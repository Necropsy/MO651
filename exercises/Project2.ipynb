{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabalho 2 - Robótica Movel - MO651\n",
    "#### __Professor(a):__ _Esther Luna Columbini_ <br>\n",
    "__Alunos:__ <br>\n",
    "_Tito Barbosa Rezende_              __RA:__ 025327<br>\n",
    "_João Paulo_                        __RA:__ 229322<br>\n",
    "_Elcio Pereira de Souza Junior_     __RA:__ 262952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-requisitos\n",
    "\n",
    "Instalação das bibliotecas necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (from matplotlib) (1.17.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (4.1.1.26)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/titobr/mestrado/robotics/notebooks/jupyter_vrep/lib/python3.6/site-packages (from opencv-python) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "sys.path.insert(0, '../src')\n",
    "from robot import Robot\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import vrep\n",
    "import math\n",
    "import multiprocessing\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Cinemático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to remoteApi server.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor1 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor2 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor3 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor4 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor5 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor6 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor7 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor8 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor9 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor10 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor11 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor12 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor13 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor14 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor15 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor16 connected.\n",
      "\u001b[92m Vision sensor connected.\n",
      "\u001b[92m Laser connected.\n",
      "\u001b[92m Left motor connected.\n",
      "\u001b[92m Right motor connected.\n",
      "\u001b[92m Robot connected.\n"
     ]
    }
   ],
   "source": [
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor Laser\n",
    "\n",
    "Nesta etapa definimos a classe para leitura dos dados que serão obtidos pelo sensor laser, e realizamos uma verificação sobre a obtenção destes dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Laser_sensor:\n",
    "    def __init__(self,robot):\n",
    "        self.robot = robot\n",
    "\n",
    "    def update_robot_frame_reading(self):\n",
    "        laser_flatten_readings = np.array(self.robot.read_laser())\n",
    "        laser_readings = laser_flatten_readings.reshape((len(laser_flatten_readings)//3, 3))\n",
    "        self.laser_x = laser_readings[:,0]\n",
    "        self.laser_y = laser_readings[:,1]\n",
    "laser_sensor = Laser_sensor(robot)\n",
    "laser_sensor.update_robot_frame_reading()\n",
    "\n",
    "def plot_lase_robot_frame():\n",
    "    fig, ax = plt.subplots()\n",
    "    #posição dos feixes laser\n",
    "    ax.scatter(laser_sensor.laser_x, laser_sensor.laser_y, 3, c='g', marker='o')\n",
    "    #posição do centro do robo\n",
    "    ax.scatter(0, 0, 40, c='b', marker='o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor Ultrassônico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter a leitura correta dos sensores de proximidade, precisamos transformar a leitura do sensor em um ponto _(x,y)_ em relação ao frame do robo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Us_sensor:\n",
    "    def __init__(self,robot):\n",
    "        orientation_angles = np.array([90,50,30,10,-10,-30,-50,-90,-90,-130,-150,-170,170,150,130,90])\n",
    "        orientation_rad = np.radians(orientation_angles)\n",
    "        self.data = {\n",
    "        \"prefix\" : \"Pioneer_p3dx_ultrasonicSensor\",\n",
    "        \"ids\" : np.arange(1,17,1), \"handles\" : np.zeros(16, dtype=int), \n",
    "        \"positions\" :  np.zeros((16,3),dtype=float),\n",
    "        \"angles_deg\": orientation_angles,\n",
    "        \"angles_rad\": orientation_rad,\n",
    "        \"raw_reading\": np.zeros(16),\n",
    "        \"robot_frame_reading\": np.zeros((16,2),dtype=float)\n",
    "        }\n",
    "        self.robot = robot\n",
    "        \n",
    "        for i,sensor_i in enumerate(self.data['ids']):\n",
    "            ret,handle = vrep.simxGetObjectHandle(self.robot.clientID, self.data['prefix'] + str(sensor_i), vrep.simx_opmode_oneshot_wait)\n",
    "            self.data['handles'][i] = handle\n",
    "            ret, pos = vrep.simxGetObjectPosition(self.robot.clientID, handle, self.robot.robot_handle, vrep.simx_opmode_oneshot_wait)\n",
    "            #ret, ang = vrep.simxGetObjectOrientation(robot.clientID, handle, robot.robot_handle, vrep.simx_opmode_oneshot_wait)\n",
    "            self.data['positions'][i,:] = pos\n",
    "            #ultrassonic_sensors[sensor_i,4:7] = ang\n",
    "            \n",
    "    def get_left_distance(self):\n",
    "        self.update_raw_reading()\n",
    "        return np.min(np.array([self.data[\"raw_reading\"][0],self.data[\"raw_reading\"][1],self.data[\"raw_reading\"][15],self.data[\"raw_reading\"][14]]))\n",
    "    \n",
    "    def get_front_distance(self):\n",
    "        self.update_raw_reading()\n",
    "        return np.min(np.array([self.data[\"raw_reading\"][2:6]]))\n",
    "    \n",
    "    def get_right_distance(self):\n",
    "        self.update_raw_reading()\n",
    "        return np.min(np.array([self.data[\"raw_reading\"][6:10]]))\n",
    "    \n",
    "    def update_raw_reading(self):\n",
    "        self.data[\"raw_reading\"] = np.array(self.robot.read_ultrassonic_sensors())\n",
    "    \n",
    "    def update_robot_frame_reading(self):\n",
    "        self.update_raw_reading()\n",
    "        for i, proximity in enumerate(us_sensors.data[\"raw_reading\"]):\n",
    "            if proximity == 5 or proximity < 0.1:\n",
    "                self.data[\"robot_frame_reading\"][i] = np.zeros(2)\n",
    "            else:\n",
    "                self.data[\"robot_frame_reading\"][i] = self.proximity_robot_frame(i+1,proximity).flatten()\n",
    "                \n",
    "        #toRobotFrame = lambda sensorId,proximity: self.proximity_robot_frame(sensorId,proximity)\n",
    "        #self.data[\"robot_frame_reading\"] = toRobotFrame(range(1,17,1),us_sensors.data[\"raw_reading\"])\n",
    "    \n",
    "    #Calcula o ponto no frame do robo, referente a leitura de cada sensor de proximidade\n",
    "    def proximity_robot_frame(self,sensorId, proximity):\n",
    "        index = sensorId -1\n",
    "        angulars = self.data[\"angles_rad\"][index]\n",
    "        #Matriz de rotação\n",
    "        rot_matrix = np.array([[math.cos(angulars),-math.sin(angulars)],[math.sin(angulars),math.cos(angulars)]])\n",
    "        #Rotacionando a leitura\n",
    "        distXY = np.dot(rot_matrix , np.array([[proximity],[0]]))\n",
    "        #Matriz de translação\n",
    "        posicao_sensor_x = self.data[\"positions\"][index][0]\n",
    "        posicao_sensor_y = self.data[\"positions\"][index][1]\n",
    "        transXY=np.array([[distXY[0][0]+posicao_sensor_x],[distXY[1][0]+posicao_sensor_y]])\n",
    "        return transXY\n",
    "\n",
    "us_sensors = Us_sensor(robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos uma função _toGlobal_ (para termos um melhor reuso de código), basicamente a função transforma um ponto qualquer _(x,y)_ que esteja na referência do robô e o leva para a referência global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toGlobal(robot_x, robot_y,robot_ang, Point_xr, Point_yr):\n",
    "    T_trans = np.array([[1,0,robot_x],[0,1,robot_y],[0,0,1]])\n",
    "    T_rot = np.array([[math.cos(robot_ang),-math.sin(robot_ang),0],[math.sin(robot_ang),math.cos(robot_ang),0],[0,0,1]])\n",
    "    T = np.dot(T_trans,T_rot)\n",
    "    res = np.dot(T, np.array([Point_xr,Point_yr,1]))\n",
    "    return res[0],res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Cinemático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado o movimento do robo, como determinar sua posição e orientação no frame global?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kinematic_model:\n",
    "    def __init__(self,robot):\n",
    "        self.robot = robot\n",
    "        \n",
    "        #Handles dos motores\n",
    "        ret1, self.motorLeft = vrep.simxGetObjectHandle(self.robot.clientID, \"Pioneer_p3dx_leftMotor\", vrep.simx_opmode_oneshot_wait)\n",
    "        ret2, self.motorRight = vrep.simxGetObjectHandle(self.robot.clientID, \"Pioneer_p3dx_rightMotor\", vrep.simx_opmode_oneshot_wait)\n",
    "        \n",
    "        #Calcula distancia de eixo\n",
    "        res, left_handle = vrep.simxGetObjectHandle(robot.clientID, \"Pioneer_p3dx_leftMotor\", vrep.simx_opmode_oneshot_wait)\n",
    "        ret, lpos = vrep.simxGetObjectPosition(robot.clientID, left_handle, robot.robot_handle, vrep.simx_opmode_oneshot_wait)\n",
    "        \n",
    "        res, right_handle = vrep.simxGetObjectHandle(robot.clientID, \"Pioneer_p3dx_rightMotor\", vrep.simx_opmode_oneshot_wait)\n",
    "        ret, rpos = vrep.simxGetObjectPosition(robot.clientID, right_handle, robot.robot_handle, vrep.simx_opmode_oneshot_wait)\n",
    "        \n",
    "        # eixo\n",
    "        self.l = (abs(lpos[1]) + abs(rpos[1]))/2\n",
    "        \n",
    "        #Ao ser criado, coleta a referencia de zero do robo\n",
    "        pos = self.robot.get_current_position()\n",
    "        self.initial_pos_x = pos[0]\n",
    "        self.initial_pos_y = pos[1]\n",
    "        orientation = self.robot.get_current_orientation()\n",
    "        self.initial_orientation = orientation[2]\n",
    "\n",
    "        #Alem de mantermos a pose inicial, manteremos a pose atualizada do robo\n",
    "        self.enc_global_x = self.initial_pos_x\n",
    "        self.enc_global_y = self.initial_pos_y\n",
    "        self.enc_Theta = self.initial_orientation\n",
    "        \n",
    "        self.time_global_x = self.initial_pos_x\n",
    "        self.time_global_y = self.initial_pos_y\n",
    "        self.time_Theta = self.initial_orientation\n",
    "        \n",
    "        #Lista de pontos para o caminho do robo\n",
    "        self.enc_path = []\n",
    "        self.time_path = []\n",
    "        self.true_path = []\n",
    "        self.update_paths()\n",
    "        \n",
    "        #inicializando a posição dos encoders\n",
    "        self.jL = self.current_encoder_left()\n",
    "        self.jR = self.current_encoder_right()\n",
    "        \n",
    "        #intervalo entre cada calculo\n",
    "        self.compute_interval = 0.1\n",
    "        self.previous_timestamp = 0\n",
    "        \n",
    "    def update_paths(self):\n",
    "        self.enc_path.append([self.enc_global_x, self.enc_global_y, self.enc_Theta])\n",
    "        self.time_path.append([self.time_global_x, self.time_global_y, self.time_Theta])\n",
    "        orientation = self.robot.get_current_orientation()\n",
    "        true_theta = orientation[2]\n",
    "        current_position = self.true_global_position()\n",
    "        self.true_path.append([current_position[0], current_position[1], true_theta])\n",
    "    \n",
    "    def true_global_position(self):\n",
    "        pos = self.robot.get_current_position()\n",
    "        return pos[0],pos[1]\n",
    "    \n",
    "    def enc_global_position(self):\n",
    "        return self.enc_global_x, self.enc_global_y, self.enc_Theta\n",
    "    \n",
    "    def time_global_position(self):\n",
    "        return self.time_global_x, self.time_global_y, self.time_Theta\n",
    "    \n",
    "    \n",
    "    ##Esta seção esta relacionada ao calculo da posição levando em consideração os encoders\n",
    "    def current_encoder_left(self):\n",
    "        ret,jL = vrep.simxGetJointPosition(self.robot.clientID,self.motorLeft,vrep.simx_opmode_oneshot_wait)\n",
    "        return jL\n",
    "    def current_encoder_right(self):\n",
    "        ret, jR = vrep.simxGetJointPosition(self.robot.clientID,self.motorRight,vrep.simx_opmode_oneshot_wait)\n",
    "        return jR\n",
    "        \n",
    "    #Phi speed of rotation of wheels\n",
    "    def Xr(self, Phi_right, Phi_left):\n",
    "        r = self.robot.WHEEL_RADIUS\n",
    "        Xr = (r*Phi_left/2) + (r*Phi_right/2)\n",
    "        return Xr\n",
    "    \n",
    "    def Theta_r(self, Phi_right, Phi_left):\n",
    "        r = self.robot.WHEEL_RADIUS\n",
    "        Tr = r*Phi_right/(2*self.l) - r*Phi_left/(2*self.l) \n",
    "        return Tr\n",
    "        \n",
    "    def speed_model(self,Phi_right,Phi_left):\n",
    "        #Se formos considerar que o eixo das rodas do robo está deslocado do eixo x\n",
    "        #return np.array([self.Xr(Phi_right,Phi_left),self.Theta_r(Phi_right,Phi_left)*self.l2,self.Theta_r(Phi_right,Phi_left)])\n",
    "        return np.array([self.Xr(Phi_right,Phi_left),0,self.Theta_r(Phi_right,Phi_left)])\n",
    "        \n",
    "    def inverse_rotation_matrix(self, ang):\n",
    "        Trot = np.array([[math.cos(ang), -math.sin(ang), 0], [math.sin(ang), math.cos(ang), 0], [0,0,1]])\n",
    "        return Trot\n",
    "    \n",
    "    def locomotion_global(self, ang, Phi_right, Phi_left):\n",
    "        return np.dot(self.inverse_rotation_matrix(ang),self.speed_model(Phi_right,Phi_left))\n",
    "    \n",
    "    def compute_with_encoder(self):\n",
    "        dxR = self.current_encoder_right() - self.jR\n",
    "        dxL = self.current_encoder_left() - self.jL\n",
    "        if (dxL>=0):\n",
    "            dxL=math.fmod(dxL+math.pi,2*math.pi)-math.pi\n",
    "        else:\n",
    "            dxL=math.fmod(dxL-math.pi,2*math.pi)+math.pi\n",
    "        if (dxR>=0):\n",
    "            dxR=math.fmod(dxR+math.pi,2*math.pi)-math.pi\n",
    "        else:\n",
    "            dxR=math.fmod(dxR-math.pi,2*math.pi)+math.pi\n",
    "        qsi = self.locomotion_global(self.enc_Theta,dxR, dxL)\n",
    "        #Atualiza a posição global\n",
    "        self.enc_global_x = self.enc_global_x + qsi[0]\n",
    "        self.enc_global_y = self.enc_global_y + qsi[1]\n",
    "        self.enc_Theta = self.enc_Theta + qsi[2]\n",
    "        #Atualiza a posição dos encoders\n",
    "        self.jR = self.current_encoder_right()\n",
    "        self.jL = self.current_encoder_left()\n",
    "    ##Fim da seção relacionada ao calculo da posição levando em consideração os encoders        \n",
    "    \n",
    "    def compute_with_time(self, Phi_right, Phi_left):\n",
    "        #Calculo do delta S\n",
    "        r = self.robot.WHEEL_RADIUS\n",
    "        Vr = r*Phi_right\n",
    "        Vl = r*Phi_left\n",
    "        current_timestamp = datetime.timestamp(datetime.now())\n",
    "        Delta_t = current_timestamp - self.previous_timestamp\n",
    "        #atualiza timestamp imediatamente\n",
    "\n",
    "        self.previous_timestamp = current_timestamp\n",
    "        \n",
    "        Delta_s = (Vr + Vl)*Delta_t/2  \n",
    "        Delta_Theta = (Vr - Vl)*Delta_t/(2*self.l)\n",
    "    \n",
    "        self.time_global_x = self.time_global_x + Delta_s*math.cos(self.time_Theta + Delta_Theta/2)\n",
    "        self.time_global_y = self.time_global_y + Delta_s*math.sin(self.time_Theta + Delta_Theta/2)\n",
    "        self.time_Theta = self.time_Theta + Delta_Theta\n",
    "    \n",
    "    def move(self,Phi_right, Phi_left,seconds): #velocidade em rad/s\n",
    "        #Vamos fixar um tempo de 500ms para computar as distâncias\n",
    "        for step in range(int(seconds/self.compute_interval)):\n",
    "            #self.compute()\n",
    "            self.robot.set_right_velocity(Phi_right)\n",
    "            self.robot.set_left_velocity(Phi_left)\n",
    "            time.sleep(self.compute_interval)\n",
    "            self.compute_with_encoder()\n",
    "            self.compute_with_time(Phi_right, Phi_left)\n",
    "            self.update_paths()\n",
    "        self.robot.stop()\n",
    "        self.timestamp = 0\n",
    "        \n",
    "    def ICR_left(self, Phi_left, R, seconds):\n",
    "        Phi_right = Phi_left*(R + self.l)/(R - self.l)\n",
    "        print(\"ICR_left Phi_r {} Phi_l {}\".format(Phi_right, Phi_left))\n",
    "        self.move(Phi_right, Phi_left, seconds)\n",
    "    \n",
    "    def ICR_right(self, Phi_right, R, seconds):\n",
    "        Phi_left = Phi_right*(R + self.l)/(R - self.l)\n",
    "        print(\"ICR_right Phi_r {} Phi_l {}\".format(Phi_right, Phi_left))\n",
    "        self.move(Phi_right, Phi_left, seconds)\n",
    "  \n",
    "    def plot_paths(self):\n",
    "        enc_path = np.array(self.enc_path)\n",
    "        time_path = np.array(self.time_path)\n",
    "        true_path = np.array(self.true_path)\n",
    "        \n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "        ax[0].scatter(enc_path[:,0], enc_path[:,1], 5, c='b', marker='o')\n",
    "        #ax[1].scatter(time_path[:,0], time_path[:,1], 5, c='g', marker='o')\n",
    "        ax[1].scatter(true_path[:,0], true_path[:,1], 5, c='r', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuvem de pontos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a nuvem de pontos, nós coletamos os dados fornecidos pelo laser (conjunto de pontos) que estão baseados no sistema de referência do robô, após aplicamos a transformação destes pontos para o sistema de referência global. Dados os pontos no sistema global nós realizamos a plotagem dos mesmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloud():\n",
    "    def __init__(self, robot, us_sensors, laser_sensor):\n",
    "        self.robot = robot\n",
    "        self.us_sensors = us_sensors\n",
    "        self.laser_sensor = laser_sensor\n",
    "        \n",
    "        self.ultrassonic_points = []\n",
    "        self.laser_points = []\n",
    "        self.robot_points = []\n",
    "    \n",
    "    def update(self):\n",
    "        #Insere posição atual do robo\n",
    "        robot_x = self.robot.get_current_position()[0]\n",
    "        robot_y = self.robot.get_current_position()[1]\n",
    "        robot_ang = self.robot.get_current_orientation()[2]\n",
    "        \n",
    "        self.robot_points.append([robot_x,robot_y])\n",
    "\n",
    "        #Atualiza a leitura do laser e insere na nuvem de pontos\n",
    "        self.laser_sensor.update_robot_frame_reading()\n",
    "        for pointx, pointy in zip(self.laser_sensor.laser_x, self.laser_sensor.laser_y):\n",
    "            x,y = toGlobal(robot_x, robot_y,robot_ang, pointx, pointy)\n",
    "            self.laser_points.append([x,y])\n",
    "\n",
    "        #Atualiza a leitura do ultrassonico e insere na nuvem de pontos\n",
    "        self.us_sensors.update_robot_frame_reading()\n",
    "        for pointx, pointy in zip(self.us_sensors.data['robot_frame_reading'][:,0], self.us_sensors.data['robot_frame_reading'][:,1]):\n",
    "            x,y = toGlobal(robot_x, robot_y,robot_ang, pointx, pointy)\n",
    "            self.ultrassonic_points.append([x,y])\n",
    "    \n",
    "    def plot_point_cloud(self):\n",
    "        #Convertendo a nuvem de pontos em um array\n",
    "        ultrassonic_point_array = np.array(self.ultrassonic_points)\n",
    "        laser_point_array = np.array(self.laser_points)\n",
    "        robot_path = np.array(self.robot_points)\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "        #posição do centro do robo\n",
    "        ax[0].scatter(robot_path[:,0], robot_path[:,1], 40, c='b', marker='o')\n",
    "        #ax[0].plot(robot_path[:,0], robot_path[:,1],'.-')\n",
    "        pass_count = 0\n",
    "        for x, y in zip(robot_path[:,0], robot_path[:,1]):\n",
    "            ax[0].text(x, y, str(pass_count), color=\"black\", fontsize=12)\n",
    "            pass_count += 1\n",
    "        #posição dos pontos lidos pelo sensor ultrassonico\n",
    "        ax[0].scatter(ultrassonic_point_array[:,0],ultrassonic_point_array[:,1], 10, c='magenta', marker='.')\n",
    "\n",
    "        #posição do centro do robo\n",
    "        ax[1].scatter(robot_path[:,0], robot_path[:,1], 40, c='b', marker='o')\n",
    "        #ax[1].plot(robot_path[:,0], robot_path[:,1],'.-')\n",
    "        pass_count = 0\n",
    "        for x, y in zip(robot_path[:,0], robot_path[:,1]):\n",
    "            ax[1].text(x, y, str(pass_count), color=\"black\", fontsize=12)\n",
    "            pass_count += 1\n",
    "        #posição dos pontos lidos pelo sensor laser\n",
    "        ax[1].scatter(laser_point_array[:,0],laser_point_array[:,1], 10, c='r', marker='.')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "        \n",
    "\n",
    "point_cloud = PointCloud(robot, us_sensors, laser_sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encapsulamento\n",
    "A classe a seguir encapsula todos os modelos criados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PID():\n",
    "    def __init__(self, P=0.5, I=0.0, D=0.0, Derivator=0, Integrator=0, Integrator_max=500, Integrator_min=-500):\n",
    "        self.Kp=P\n",
    "        self.Ki=I\n",
    "        self.Kd=D\n",
    "        self.Derivator=Derivator\n",
    "        self.Integrator=Integrator\n",
    "        self.Integrator_max=Integrator_max\n",
    "        self.Integrator_min=Integrator_min\n",
    "        self.set_point=0.0\n",
    "        self.error=0.0\n",
    "\n",
    "    def update(self,current_value):\n",
    "        self.error = self.set_point - current_value\n",
    "        self.P_value = self.Kp * self.error\n",
    "        self.D_value = self.Kd * ( self.error - self.Derivator)\n",
    "        self.Derivator = self.error\n",
    "        self.Integrator = self.Integrator + self.error\n",
    "\n",
    "        if self.Integrator > self.Integrator_max:\n",
    "            self.Integrator = self.Integrator_max\n",
    "        elif self.Integrator < self.Integrator_min:\n",
    "            self.Integrator = self.Integrator_min\n",
    "\n",
    "        self.I_value = self.Integrator * self.Ki\n",
    "\n",
    "        PID = self.P_value + self.I_value + self.D_value\n",
    "        print(\"PID cv \",current_value ,\" sp \", self.set_point ,\" error \", self.error ,\" pid \", PID)\n",
    "        return PID\n",
    "\n",
    "    def setPoint(self,set_point):\n",
    "        self.set_point = set_point\n",
    "        self.Integrator=0\n",
    "        self.Derivator=0\n",
    "\n",
    "# \tdef setIntegrator(self, Integrator):\n",
    "# \t\tself.Integrator = Integrator\n",
    "\n",
    "# \tdef setDerivator(self, Derivator):\n",
    "# \t\tself.Derivator = Derivator\n",
    "\n",
    "# \tdef setKp(self,P):\n",
    "# \t\tself.Kp=P\n",
    "\n",
    "# \tdef setKi(self,I):\n",
    "# \t\tself.Ki=I\n",
    "\n",
    "# \tdef setKd(self,D):\n",
    "# \t\tself.Kd=D\n",
    "\n",
    "# \tdef getPoint(self):\n",
    "# \t\treturn self.set_point\n",
    "\n",
    "# \tdef getError(self):\n",
    "# \t\treturn self.error\n",
    "\n",
    "# \tdef getIntegrator(self):\n",
    "# \t\treturn self.Integrator\n",
    "\n",
    "# \tdef getDerivator(self):\n",
    "# \t\treturn self.Derivator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to remoteApi server.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor1 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor2 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor3 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor4 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor5 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor6 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor7 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor8 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor9 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor10 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor11 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor12 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor13 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor14 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor15 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor16 connected.\n",
      "\u001b[92m Vision sensor connected.\n",
      "\u001b[92m Laser connected.\n",
      "\u001b[92m Left motor connected.\n",
      "\u001b[92m Right motor connected.\n",
      "\u001b[92m Robot connected.\n"
     ]
    }
   ],
   "source": [
    "class MobileRobot():\n",
    "    def __init__(self):\n",
    "        self.robot = Robot()\n",
    "        self.kinematicModel = Kinematic_model(self.robot)\n",
    "        self.us_sensors = Us_sensor(self.robot)\n",
    "        self.laser_sensors = Laser_sensor(self.robot)\n",
    "        self.pidL = PID()\n",
    "        self.pidL.setPoint(0.5)\n",
    "#         self.pidF = PID()\n",
    "#         self.pidF.setPoint(1)\n",
    "        self.compute_interval = 0.5\n",
    "        self.follow_wall_side = \"LEFT\"\n",
    "        \n",
    "    def control(self, left_distance, right_distance, front_distance ):\n",
    "        left = self.us_sensors.get_left_distance()\n",
    "        right = self.us_sensors.get_right_distance()\n",
    "        front = self.us_sensors.get_front_distance()\n",
    "        \n",
    "        print(\"left \", left, \"right \", right, \"front \", front)\n",
    "        \n",
    "        #State Machine\n",
    "        ##Move foward when there is no wall to follow\n",
    "        if (left == 5 and right == 5 and front > 0.5):\n",
    "            print(\"move foward\")\n",
    "            turn = 0\n",
    "            vel = 1\n",
    "        ##Turn away from wall\n",
    "        elif (front <= 0.5):\n",
    "            print(\"turn away from wall\")\n",
    "            turn = 1\n",
    "            vel = 0\n",
    "        else:\n",
    "            print(\"follow wall\")\n",
    "            turn = self.pidL.update(left_distance)\n",
    "            vel = 1\n",
    "        return vel + turn, vel - turn\n",
    "\n",
    "    def start(self, seconds):\n",
    "        for step in range(int(seconds/self.compute_interval)):\n",
    "            #Phi_l, Phi_r = self.braitenberg(self.us_sensors.data[\"raw_reading\"][:8],2)\n",
    "            Phi_l, Phi_r = self.control(self.us_sensors.get_left_distance(),self.us_sensors.get_right_distance(), self.us_sensors.get_front_distance())\n",
    "            self.kinematicModel.move(Phi_r, Phi_l,self.compute_interval)\n",
    "            #self.kinematicModel.plot_paths()\n",
    "            #self.point_cloud.update()\n",
    "            #self.point_cloud.plot_point_cloud()\n",
    "mr = MobileRobot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573866486549377"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.us_sensors.get_left_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3223384916782379"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.us_sensors.get_front_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left  0.8573809266090393 right  0.34594401717185974 front  0.3223384916782379\n",
      "turn away from wall\n",
      "left  0.3483438789844513 right  5.0 front  0.32911399006843567\n",
      "turn away from wall\n",
      "left  0.3345804810523987 right  5.0 front  0.40069007873535156\n",
      "turn away from wall\n",
      "left  0.28920865058898926 right  0.7934370040893555 front  5.0\n",
      "follow wall\n",
      "PID cv  0.28920865058898926  sp  0.5  error  0.21079134941101074  pid  0.10539567470550537\n",
      "left  0.3596184253692627 right  0.9062818288803101 front  0.9091351628303528\n",
      "follow wall\n",
      "PID cv  0.3596184253692627  sp  0.5  error  0.1403815746307373  pid  0.07019078731536865\n",
      "left  0.4468975365161896 right  5.0 front  0.7898662686347961\n",
      "follow wall\n",
      "PID cv  0.4468975365161896  sp  0.5  error  0.053102463483810425  pid  0.026551231741905212\n",
      "left  0.541679859161377 right  5.0 front  0.6784231066703796\n",
      "follow wall\n",
      "PID cv  0.541679859161377  sp  0.5  error  -0.04167985916137695  pid  -0.020839929580688477\n",
      "left  0.584722101688385 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.584722101688385  sp  0.5  error  -0.08472210168838501  pid  -0.042361050844192505\n",
      "left  0.49073001742362976 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.49073001742362976  sp  0.5  error  0.00926998257637024  pid  0.00463499128818512\n",
      "left  0.47318944334983826 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.47318944334983826  sp  0.5  error  0.026810556650161743  pid  0.013405278325080872\n",
      "left  0.39281997084617615 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.39281997084617615  sp  0.5  error  0.10718002915382385  pid  0.053590014576911926\n",
      "left  0.39320385456085205 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.39320385456085205  sp  0.5  error  0.10679614543914795  pid  0.053398072719573975\n",
      "left  0.3855316638946533 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.3855316638946533  sp  0.5  error  0.11446833610534668  pid  0.05723416805267334\n",
      "left  0.4352892339229584 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.4352892339229584  sp  0.5  error  0.06471076607704163  pid  0.03235538303852081\n",
      "left  0.5273984670639038 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.5273984670639038  sp  0.5  error  -0.02739846706390381  pid  -0.013699233531951904\n",
      "left  0.6315723657608032 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.6315723657608032  sp  0.5  error  -0.13157236576080322  pid  -0.06578618288040161\n",
      "left  0.7413011193275452 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.7413011193275452  sp  0.5  error  -0.24130111932754517  pid  -0.12065055966377258\n",
      "left  0.8640168905258179 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.8640168905258179  sp  0.5  error  -0.36401689052581787  pid  -0.18200844526290894\n",
      "left  0.9948745965957642 right  5.0 front  5.0\n",
      "follow wall\n",
      "PID cv  0.9948745965957642  sp  0.5  error  -0.49487459659576416  pid  -0.24743729829788208\n",
      "left  5.0 right  5.0 front  5.0\n",
      "move foward\n",
      "left  5.0 right  5.0 front  5.0\n",
      "move foward\n"
     ]
    }
   ],
   "source": [
    "mr.start(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.kinematicModel.move(-1, -1, 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "<br>\n",
    "Para o projeto ultilizamos os seguintes sensores:\n",
    "\n",
    "* Encoders\n",
    "* Sensores Ultrassônicos\n",
    "* Laser Hokuyo\n",
    "* Câmera\n",
    "<br><br>\n",
    "<div style=\"text-align: justify\">\n",
    "Os encoders foram utilizados para o processo de odometria, com os dados fornecidos por eles nós estimamos o deslocamento realizado pelo robô no ambiente. Já os sensores ultrassônicos utilizamos para detectar objetos que estivessem mais próximos do robô devido seu curto alcance, porém focamos na utilização do laser para criação da nuvem de pontos por apresentar uma maior precisão e alcance. A utilização da câmera durante o processo foi especificamente para depuração do código, durante a construção do mesmo utilizamos ela para ter uma melhor visão do que estava sendo observado pelo robô.\n",
    "Como mencionado anteriormente os dados obtidos durante a coleta apresentam algumas variações em relação aos dados reais fornecidos pelo simulador V-REP (variações visíveis na odometria), estas variações são mais perceptíveis quando principalmente o robô realiza mudanças de direção, acreditamos que isto deve ocorrer pelo fato da imprecisão dos sensores (neste caso o encoder), o que gera um erro durante os cálculos de estimação de posição, tendo assim um acúmulo de erros durante todo o processo. Uma alternativa para a correção deste erro é a de utilização dos dados do laser como alternativa para minimização destes erros, onde poderíamos pegar pontos de referência na nuvem de pontos para calcularmos os ângulos de rotação e o deslocamento, porém o aperfeiçoamento deste modelo deve ser realizado somente na próxima etapa do trabalho.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
